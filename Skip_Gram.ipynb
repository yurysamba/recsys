{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b031919e",
   "metadata": {
    "gather": {
     "logged": 1670431738951
    }
   },
   "source": [
    "# Skip Gram \n",
    "- This is an implementation of Mikolov's paper: Distributed Representations of Words and Phrases and their Compositionality for any target/context items\n",
    "- In this example, the dataset that is used contains numbers. These number actually represent set of items that were searched and sometimes purchased within the same web session. \n",
    "- In word2vec, a representation is learned for words that are seen together (in the same context). In this implementation of word2vec, we learn a representation for items that are seen in the same context. Context = web_session and our words = items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecb3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38993032-d326-48a7-9511-3dc10cfddd31",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. Import Dataset\n",
    "- The dataset should contain a set of input, output data\n",
    "- There should be as many rows as occurence of these pairs of target, context items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44d3abc0-0114-4946-aece-82c6a27be077",
   "metadata": {
    "gather": {
     "logged": 1670431756071
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698928\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1301690</th>\n",
       "      <td>922</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736268</th>\n",
       "      <td>922</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212010</th>\n",
       "      <td>922</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373414</th>\n",
       "      <td>1307</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896903</th>\n",
       "      <td>799</td>\n",
       "      <td>1413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         input  output\n",
       "1301690    922     172\n",
       "736268     922     265\n",
       "1212010    922     970\n",
       "1373414   1307     900\n",
       "896903     799    1413"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The dataset should contain the target and context word. \n",
    "An example of the format is:\n",
    "        input, output\n",
    "        toad, amphibian\n",
    "        frog, amphibian\n",
    "        amphibian, toad\n",
    "        amphibian, frog\n",
    "        politics, paris\n",
    "        paris, politics\n",
    "        ...\n",
    "\"\"\"\n",
    "\n",
    "emb = pd.read_csv('df_sample.csv') # Import your dataset\n",
    "emb = emb.sample(frac=1) # Shuffling the data to be able to shuffle a smaller size when creating the tensorflow dataset\n",
    "print(len(emb))\n",
    "emb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c2012",
   "metadata": {},
   "source": [
    "### 2. Data Processing\n",
    "- Rare word pruning: removing certain pairs of target/context items because their occurence is low. These pairs of items that do not appear often enough will bring noise and the model will not learn a meaningful relationship between for these items.\n",
    "- Subsampling: if we do not subsample certain pairs of items, we will work an unbalanced dataset. We will have a model that is overly focused on certain pairs of items. Instead, it's best to discard these pairs of items for which we have too many occurences. \n",
    "- Negative Sampling: The Vanilla version of the Skip-Gram model does not work well in practice. It is extremely expensive to have to compute the softmax function for all words in the vocabulary/ items in the catalog. Instead, one way to overcome this is to change the task from a multiclass classification problem to a binary classification problem. For each target word, we select n negative samples where the context word is selected with a certain probability from the entire dataset.\n",
    "\n",
    "- More details about each of these function can be found in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4371797c-479a-46c0-ab0b-38029cce9af6",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 2.1 Rare Word Pruning\n",
    "\n",
    "Minimum number of words to learn a meaningful representation for the pair of context/target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "374ea624-c7d6-4e0a-97e5-3477e379fe01",
   "metadata": {
    "gather": {
     "logged": 1670431787529
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a treshold of 100: 0.0% of observations were pruned\n",
      "2698928\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input  output\n",
       "0      0     693\n",
       "1      0     693\n",
       "2      0     693\n",
       "3      0     693\n",
       "4      0     693"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_count_rare_word_pruning = 100\n",
    "emb2 = prune_rare_words(emb, min_count_rare_word_pruning)\n",
    "print(len(emb2))\n",
    "emb2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07167d0-c690-4aa8-94b8-286e71ef3dc0",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 2.2 Subsampling\n",
    "Certain pairs of words appear more often then others. \n",
    "These pairs of words should not be over-represented in our dataset otherwise we will be working with an unbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5be801df-0e31-4bd5-bc20-e2fb83688473",
   "metadata": {
    "gather": {
     "logged": 1670431853887
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341762\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78736</th>\n",
       "      <td>317.0</td>\n",
       "      <td>528.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275794</th>\n",
       "      <td>1143.0</td>\n",
       "      <td>1149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266472</th>\n",
       "      <td>1064.0</td>\n",
       "      <td>843.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42373</th>\n",
       "      <td>246.0</td>\n",
       "      <td>265.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41061</th>\n",
       "      <td>222.0</td>\n",
       "      <td>620.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         input  output\n",
       "78736    317.0   528.0\n",
       "275794  1143.0  1149.0\n",
       "266472  1064.0   843.0\n",
       "42373    246.0   265.0\n",
       "41061    222.0   620.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsampling_treshold = 0.0002\n",
    "emb3 = subsample(emb2, subsampling_treshold).sample(frac=1) # to make sure that we shuffle correctly\n",
    "print(len(emb3))\n",
    "emb3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b80a3ed2-82d6-49fe-ae8d-99861747e20f",
   "metadata": {
    "gather": {
     "logged": 1670431867067
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "observed_items = emb3.values.flatten() # returns all observed items in a list \n",
    "vocab_size = len(set(observed_items)) # Will need to change that for number of distinct items in dataset\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "607a3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, inv_vocab = create_vocab(emb3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9fc0f07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78736</th>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275794</th>\n",
       "      <td>47</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266472</th>\n",
       "      <td>22</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42373</th>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41061</th>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        input  output\n",
       "78736      28       8\n",
       "275794     47      49\n",
       "266472     22      35\n",
       "42373      46       5\n",
       "41061      39      42"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb3['input'] = emb3.input.apply(lambda x: vocab[x])\n",
    "emb3['output'] = emb3.output.apply(lambda x: vocab[x])\n",
    "emb3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50687f-88a7-4b13-9989-54b8d0c1d5b8",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 2.3 Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4f035e4e-714f-4c13-bd43-adbc78102844",
   "metadata": {
    "gather": {
     "logged": 1670432920828
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For example: item 0 and item 1 represents a true pair of items found in our dataset. \n",
    "We will generate 3 negative samples by selecting at random with a certain probability other items:\n",
    "\n",
    "- input, output, y \n",
    "- 0, 1, 1\n",
    "- 0, 2, 0\n",
    "- 0, 4, 0\n",
    "- 0, 7, 0\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "pwr_val=1/4\n",
    "\n",
    "neg_samp_df, targets, contexts, labels = generate_negative_samples(emb3.sample(frac=1), k, pwr_val)\n",
    "targets_tf = tf.convert_to_tensor(targets)\n",
    "contexts_tf = tf.convert_to_tensor(contexts)\n",
    "labels_tf = tf.convert_to_tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b06a17c6-7fbb-422f-9818-9d54b463be82",
   "metadata": {
    "gather": {
     "logged": 1670432937925
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "## FREE UP MEMORY\n",
    "del emb, emb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70d7cb",
   "metadata": {},
   "source": [
    "### 3. Generate Tensorflow Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "161a005a-a235-4e0a-8926-816a2fc73758",
   "metadata": {
    "gather": {
     "logged": 1670435780992
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:  307585 Test Size: 34177\n"
     ]
    }
   ],
   "source": [
    "# Just disables the warning, doesn't take advantage of AVX/FMA to run faster\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # this is to ignore the error message: assuming you are running on GPU it is not necessary to see the error message\n",
    "\n",
    "BATCH_SIZE = 30  ### --> HYPER-PARAMETER TO FINE TUNE\n",
    "BUFFER_SIZE = 10000\n",
    "TRAIN_PERC = .9 # Percentage of samples in train\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets_tf, contexts_tf), labels_tf))\n",
    "n_samples = len(list(dataset))\n",
    "train_size = int(n_samples * TRAIN_PERC)\n",
    "test_size = n_samples - train_size\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)\n",
    "\n",
    "print(\"Train Size: \", len(list(train_dataset)), \"Test Size:\", len(list(test_dataset)))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0b44508-e618-4637-b7c8-107723a736db",
   "metadata": {
    "gather": {
     "logged": 1670435781140
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "filename_ = str(min_count_rare_word_pruning) + \"_\" + str(subsampling_treshold) + \"_\" + str(vocab_size) + \"_\" + str(k) + \"_\" + str(pwr_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f292a1",
   "metadata": {},
   "source": [
    "### 4. Define the Hyperparameters of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd800296",
   "metadata": {},
   "source": [
    "- LEARNING_RATES: The learning rate of the optimizer\n",
    "- EMBEDDING_DIMS: The dimension of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aggressive-label",
   "metadata": {
    "gather": {
     "logged": 1670435781523
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIMS = [5,6,7] #[10\n",
    "LEARNING_RATES = [.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "generous-committee",
   "metadata": {
    "gather": {
     "logged": 1669689242657
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10252/10252 [==============================] - 12s 1ms/step - loss: 0.3662 - accuracy: 0.9058 - val_loss: 0.1984 - val_accuracy: 0.9270\n",
      "Epoch 2/10\n",
      "10252/10252 [==============================] - 11s 1ms/step - loss: 0.1894 - accuracy: 0.9296 - val_loss: 0.1879 - val_accuracy: 0.9287\n",
      "Epoch 3/10\n",
      "10252/10252 [==============================] - 14s 1ms/step - loss: 0.1856 - accuracy: 0.9296 - val_loss: 0.1870 - val_accuracy: 0.9294\n",
      "Epoch 4/10\n",
      "10252/10252 [==============================] - 11s 1ms/step - loss: 0.1850 - accuracy: 0.9293 - val_loss: 0.1865 - val_accuracy: 0.9285\n",
      "1139/1139 [==============================] - 1s 885us/step - loss: 0.1880 - accuracy: 0.9287\n",
      "5-0.001 Test Loss 0.1879599690437317 Test Accuracy 0.9287386536598206\n",
      "Epoch 1/10\n",
      "10252/10252 [==============================] - 11s 1ms/step - loss: 0.3395 - accuracy: 0.9128 - val_loss: 0.1940 - val_accuracy: 0.9282\n",
      "Epoch 2/10\n",
      "10252/10252 [==============================] - 11s 1ms/step - loss: 0.1876 - accuracy: 0.9294 - val_loss: 0.1877 - val_accuracy: 0.9277\n",
      "Epoch 3/10\n",
      "10252/10252 [==============================] - 11s 1ms/step - loss: 0.1852 - accuracy: 0.9293 - val_loss: 0.1865 - val_accuracy: 0.9300\n",
      "Epoch 4/10\n",
      "10252/10252 [==============================] - 12s 1ms/step - loss: 0.1848 - accuracy: 0.9294 - val_loss: 0.1866 - val_accuracy: 0.9271\n",
      "Epoch 5/10\n",
      "10252/10252 [==============================] - 11s 1ms/step - loss: 0.1846 - accuracy: 0.9289 - val_loss: 0.1861 - val_accuracy: 0.9285\n",
      "1139/1139 [==============================] - 1s 952us/step - loss: 0.1865 - accuracy: 0.9300\n",
      "6-0.001 Test Loss 0.18649323284626007 Test Accuracy 0.9299678206443787\n",
      "Epoch 1/10\n",
      "10252/10252 [==============================] - 12s 1ms/step - loss: 0.3214 - accuracy: 0.9172 - val_loss: 0.1904 - val_accuracy: 0.9289\n",
      "Epoch 2/10\n",
      "10252/10252 [==============================] - 12s 1ms/step - loss: 0.1862 - accuracy: 0.9291 - val_loss: 0.1873 - val_accuracy: 0.9286\n",
      "Epoch 3/10\n",
      "10252/10252 [==============================] - 11s 1ms/step - loss: 0.1850 - accuracy: 0.9290 - val_loss: 0.1867 - val_accuracy: 0.9277\n",
      "1139/1139 [==============================] - 2s 1ms/step - loss: 0.1904 - accuracy: 0.9289\n",
      "7-0.001 Test Loss 0.19042019546031952 Test Accuracy 0.9288557171821594\n"
     ]
    }
   ],
   "source": [
    "save_embeddings = True\n",
    "for embedding_dim in EMBEDDING_DIMS: \n",
    "    for learning_rate in LEARNING_RATES: \n",
    "\n",
    "        train_loss_dict = {}\n",
    "        train_accuracy_dict = {}\n",
    "        test_results_dict = {}\n",
    "\n",
    "        word2vec = Word2Vec(vocab_size, embedding_dim, k)\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "                learning_rate=learning_rate,\n",
    "                beta_1=0.9,\n",
    "                beta_2=0.999,\n",
    "                epsilon=1e-07,\n",
    "                amsgrad=False,\n",
    "                name='Adam',\n",
    "            )\n",
    "            \n",
    "        word2vec.compile(optimizer=opt,\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "        \n",
    "        # Callbacks\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                    min_delta=0.001,\n",
    "                    patience=2,\n",
    "                    mode='max',\n",
    "                    restore_best_weights=True) \n",
    "        \n",
    "        results = word2vec.fit(train_dataset, \n",
    "                                epochs=10, \n",
    "                                callbacks=[tensorboard_callback, es],\n",
    "                                validation_data=test_dataset\n",
    "                            )                     \n",
    "        \n",
    "        model_name = f\"{str(embedding_dim)}-{str(learning_rate)}\"\n",
    "        train_loss_dict[model_name] = results.history[\"loss\"]\n",
    "        train_accuracy_dict[model_name] = results.history[\"accuracy\"]\n",
    "        \n",
    "        test_results = word2vec.evaluate(test_dataset, callbacks=[tensorboard_callback])\n",
    "        test_results_dict[model_name] = test_results \n",
    "\n",
    "        model_id = filename_ + '_EMB_DIM_' + str(embedding_dim) + \"_LR_\" + str(learning_rate)\n",
    "\n",
    "        logs_dict = {model_id:{'loss':train_loss_dict, \n",
    "                        'accuracy':train_accuracy_dict,\n",
    "                        'test':test_results_dict\n",
    "                                    }\n",
    "                    }\n",
    "\n",
    "        # Saving as a pickle file\n",
    "        local_path = './skip_gram_training_logs/' + model_id + '/'\n",
    "\n",
    "        # if the path doesn't exist\n",
    "        if not os.path.exists(local_path):\n",
    "            os.makedirs(local_path)\n",
    "        \n",
    "        with open(local_path + 'logs_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(logs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        print(model_name, \"Test Loss\", test_results[0], \"Test Accuracy\", test_results[1])\n",
    "\n",
    "        if save_embeddings:\n",
    "            weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "            df_embed = pd.DataFrame(weights, index=vocab.keys())\n",
    "\n",
    "            with open(local_path + 'df_embed.pickle', 'wb') as handle:\n",
    "                pickle.dump(df_embed, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c26ee-dcfe-4b57-b31f-9b8efaf241d1",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 5. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96838938",
   "metadata": {},
   "source": [
    "#### 5.1 Extracting the weights as the representation of our item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fe5e72ec-5908-427c-bff8-ad86ee9cfd58",
   "metadata": {
    "gather": {
     "logged": 1669689242789
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "df_embed = pd.DataFrame(weights, index = vocab.keys())\n",
    "df_embed.index = df_embed.index.astype(int).astype(str)\n",
    "df_embed.to_csv(\"./df_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f4fae",
   "metadata": {},
   "source": [
    "#### 5.2 Using cosine similarity to identify most similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb7511f8-2d73-474e-91e5-406261f4ff67",
   "metadata": {
    "gather": {
     "logged": 1669744753958
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1016</th>\n",
       "      <th>1024</th>\n",
       "      <th>104</th>\n",
       "      <th>1044</th>\n",
       "      <th>1064</th>\n",
       "      <th>1085</th>\n",
       "      <th>113</th>\n",
       "      <th>1143</th>\n",
       "      <th>1149</th>\n",
       "      <th>...</th>\n",
       "      <th>801</th>\n",
       "      <th>843</th>\n",
       "      <th>882</th>\n",
       "      <th>900</th>\n",
       "      <th>916</th>\n",
       "      <th>918</th>\n",
       "      <th>922</th>\n",
       "      <th>923</th>\n",
       "      <th>958</th>\n",
       "      <th>970</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.179694</td>\n",
       "      <td>-0.550086</td>\n",
       "      <td>-0.761748</td>\n",
       "      <td>-0.455697</td>\n",
       "      <td>0.047056</td>\n",
       "      <td>0.026907</td>\n",
       "      <td>-0.186893</td>\n",
       "      <td>0.068238</td>\n",
       "      <td>-0.405951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491805</td>\n",
       "      <td>-0.330699</td>\n",
       "      <td>0.011031</td>\n",
       "      <td>-0.440332</td>\n",
       "      <td>-0.537115</td>\n",
       "      <td>-0.447851</td>\n",
       "      <td>0.262245</td>\n",
       "      <td>0.273086</td>\n",
       "      <td>0.646168</td>\n",
       "      <td>0.175818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>-0.179694</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.210466</td>\n",
       "      <td>0.432494</td>\n",
       "      <td>-0.063091</td>\n",
       "      <td>-0.418049</td>\n",
       "      <td>0.149768</td>\n",
       "      <td>0.999928</td>\n",
       "      <td>-0.607989</td>\n",
       "      <td>-0.057850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.401600</td>\n",
       "      <td>0.540393</td>\n",
       "      <td>0.255535</td>\n",
       "      <td>-0.078993</td>\n",
       "      <td>-0.388288</td>\n",
       "      <td>-0.068592</td>\n",
       "      <td>0.289697</td>\n",
       "      <td>-0.388918</td>\n",
       "      <td>-0.295232</td>\n",
       "      <td>-0.366808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>-0.550086</td>\n",
       "      <td>0.210466</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.374160</td>\n",
       "      <td>-0.311959</td>\n",
       "      <td>-0.474904</td>\n",
       "      <td>0.187449</td>\n",
       "      <td>0.219032</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.337302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.396782</td>\n",
       "      <td>0.715417</td>\n",
       "      <td>-0.346482</td>\n",
       "      <td>-0.329246</td>\n",
       "      <td>0.383749</td>\n",
       "      <td>-0.317004</td>\n",
       "      <td>-0.480472</td>\n",
       "      <td>-0.567409</td>\n",
       "      <td>-0.340935</td>\n",
       "      <td>-0.343331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-0.761748</td>\n",
       "      <td>0.432494</td>\n",
       "      <td>0.374160</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.397086</td>\n",
       "      <td>-0.331633</td>\n",
       "      <td>0.465520</td>\n",
       "      <td>0.436717</td>\n",
       "      <td>-0.065275</td>\n",
       "      <td>0.284987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542684</td>\n",
       "      <td>0.321466</td>\n",
       "      <td>-0.319208</td>\n",
       "      <td>0.391498</td>\n",
       "      <td>0.467739</td>\n",
       "      <td>0.389218</td>\n",
       "      <td>0.221026</td>\n",
       "      <td>0.036478</td>\n",
       "      <td>-0.934996</td>\n",
       "      <td>-0.452998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>-0.455697</td>\n",
       "      <td>-0.063091</td>\n",
       "      <td>-0.311959</td>\n",
       "      <td>0.397086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373983</td>\n",
       "      <td>0.063309</td>\n",
       "      <td>-0.064850</td>\n",
       "      <td>-0.094470</td>\n",
       "      <td>0.327979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172992</td>\n",
       "      <td>-0.369894</td>\n",
       "      <td>0.211554</td>\n",
       "      <td>0.999527</td>\n",
       "      <td>0.335488</td>\n",
       "      <td>0.999885</td>\n",
       "      <td>0.171301</td>\n",
       "      <td>0.256277</td>\n",
       "      <td>-0.171632</td>\n",
       "      <td>-0.163779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1016      1024       104      1044      1064      1085  \\\n",
       "0     1.000000 -0.179694 -0.550086 -0.761748 -0.455697  0.047056  0.026907   \n",
       "1016 -0.179694  1.000000  0.210466  0.432494 -0.063091 -0.418049  0.149768   \n",
       "1024 -0.550086  0.210466  1.000000  0.374160 -0.311959 -0.474904  0.187449   \n",
       "104  -0.761748  0.432494  0.374160  1.000000  0.397086 -0.331633  0.465520   \n",
       "1044 -0.455697 -0.063091 -0.311959  0.397086  1.000000  0.373983  0.063309   \n",
       "\n",
       "           113      1143      1149  ...       801       843       882  \\\n",
       "0    -0.186893  0.068238 -0.405951  ...  0.491805 -0.330699  0.011031   \n",
       "1016  0.999928 -0.607989 -0.057850  ... -0.401600  0.540393  0.255535   \n",
       "1024  0.219032  0.189655  0.337302  ... -0.396782  0.715417 -0.346482   \n",
       "104   0.436717 -0.065275  0.284987  ... -0.542684  0.321466 -0.319208   \n",
       "1044 -0.064850 -0.094470  0.327979  ...  0.172992 -0.369894  0.211554   \n",
       "\n",
       "           900       916       918       922       923       958       970  \n",
       "0    -0.440332 -0.537115 -0.447851  0.262245  0.273086  0.646168  0.175818  \n",
       "1016 -0.078993 -0.388288 -0.068592  0.289697 -0.388918 -0.295232 -0.366808  \n",
       "1024 -0.329246  0.383749 -0.317004 -0.480472 -0.567409 -0.340935 -0.343331  \n",
       "104   0.391498  0.467739  0.389218  0.221026  0.036478 -0.934996 -0.452998  \n",
       "1044  0.999527  0.335488  0.999885  0.171301  0.256277 -0.171632 -0.163779  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.spatial as sp\n",
    "\n",
    "df_embed.sort_index(inplace=True)\n",
    "#computing the cosine between the items\n",
    "cosines = 1 - sp.distance.cdist(df_embed.values, df_embed.values, 'cosine')\n",
    "\n",
    "#creating a dataframe from the cosine\n",
    "df_cosines = pd.DataFrame(cosines, columns = df_embed.index, index = df_embed.index)\n",
    "\n",
    "df_cosines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ed0e8e4d-9fa3-4d59-a9d7-eb27e822d026",
   "metadata": {
    "gather": {
     "logged": 1669744759613
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>cosine</th>\n",
       "      <th>similar_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.646168</td>\n",
       "      <td>958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.539889</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.491805</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.486924</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  item    cosine similar_item\n",
       "0    0  1.000000            0\n",
       "1    0  0.646168          958\n",
       "2    0  0.539889          246\n",
       "3    0  0.491805          801\n",
       "4    0  0.486924          300"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the top 10 most similar items for each item\n",
    "big_arr = []\n",
    "for row in df_cosines.iterrows():\n",
    "    \n",
    "    top10 = row[1].nlargest(10)\n",
    "    \n",
    "    for i in range(0, len(top10.values)):\n",
    "        big_arr.append([top10.name, top10.values[i], top10.index.values[i]])\n",
    "    \n",
    "    \n",
    "df_similarity = pd.DataFrame(big_arr, columns=['item', 'cosine', 'similar_item'])\n",
    "df_similarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3cd20589-f064-4804-9e27-5b8d0b2953f6",
   "metadata": {
    "gather": {
     "logged": 1669745919524
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>cosine</th>\n",
       "      <th>similar_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.646168</td>\n",
       "      <td>958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.539889</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.491805</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.486924</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.396434</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.379946</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.319913</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.273086</td>\n",
       "      <td>923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.262245</td>\n",
       "      <td>922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  item    cosine similar_item\n",
       "0    0  1.000000            0\n",
       "1    0  0.646168          958\n",
       "2    0  0.539889          246\n",
       "3    0  0.491805          801\n",
       "4    0  0.486924          300\n",
       "5    0  0.396434          732\n",
       "6    0  0.379946          150\n",
       "7    0  0.319913          562\n",
       "8    0  0.273086          923\n",
       "9    0  0.262245          922"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to find other similar items\n",
    "df_similarity[df_similarity.item=='0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf26c54",
   "metadata": {},
   "source": [
    "#### 5.3 Extracting the representation for each item to display them in the embedding projector\n",
    "- https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "119901ba-a9ea-46c9-927e-67a695992761",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "weights = {}\n",
    "\n",
    "for idx, row in df_embed.iterrows():\n",
    "    weights[idx] = row.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "38588e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8c9c9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for key, val in weights.items(): # for index, word in enumerate(vocab):\n",
    "  vec = weights[key]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(key + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cdf51c",
   "metadata": {},
   "source": [
    "### 6. Additional steps performed not displayed here\n",
    "1. Evaluation of the best model by looking at precision and recall for the different models generated. By different models, I mean the ones that are generated based on the different hyper-parameters (embedding size, learning rate, number of negative samples, subsampling rate, etc.)\n",
    "2. Comparison to other benchmarks"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
