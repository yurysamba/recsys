{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e1a9aa9",
   "metadata": {
    "gather": {
     "logged": 1670431738951
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38993032-d326-48a7-9511-3dc10cfddd31",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. Import Dataset That Contains the Input, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44d3abc0-0114-4946-aece-82c6a27be077",
   "metadata": {
    "gather": {
     "logged": 1670431756071
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>714313</th>\n",
       "      <td>236</td>\n",
       "      <td>813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84432</th>\n",
       "      <td>693</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49321</th>\n",
       "      <td>433</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530769</th>\n",
       "      <td>562</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749940</th>\n",
       "      <td>1343</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        input  output\n",
       "714313    236     813\n",
       "84432     693     288\n",
       "49321     433     620\n",
       "530769    562     336\n",
       "749940   1343    1249"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The dataset should contain the target and context word. \n",
    "An example of the format is:\n",
    "        input, output\n",
    "        toad, amphibian\n",
    "        frog, amphibian\n",
    "        amphibian, toad\n",
    "        amphibian, frog\n",
    "        politics, paris\n",
    "        paris, politics\n",
    "        ...\n",
    "\"\"\"\n",
    "\n",
    "emb = pd.read_csv('df.csv').head(1000000) # Import your dataset\n",
    "emb = emb.sample(frac=1) # Shuffling the data to be able to shuffle a smaller size when creating the tensorflow dataset\n",
    "emb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c2012",
   "metadata": {},
   "source": [
    "### 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4371797c-479a-46c0-ab0b-38029cce9af6",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 2.1 Rare Word Pruning\n",
    "\n",
    "Minimum number of words to learn a meaningful representation for the pair of context/target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "374ea624-c7d6-4e0a-97e5-3477e379fe01",
   "metadata": {
    "gather": {
     "logged": 1670431787529
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a treshold of 10: 17.51% of observations were pruned\n",
      "824904\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input  output\n",
       "0      0    1006\n",
       "1      0    1006\n",
       "2      0    1006\n",
       "3      0    1006\n",
       "4      0    1006"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_count_rare_word_pruning = 10 #100\n",
    "emb2 = prune_rare_words(emb, min_count_rare_word_pruning)\n",
    "print(len(emb2))\n",
    "emb2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07167d0-c690-4aa8-94b8-286e71ef3dc0",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 2.2 Subsampling\n",
    "Certain pairs of words appear more often then others. \n",
    "These pairs of words should not be over-represented in our dataset otherwise we will be working with an unbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5be801df-0e31-4bd5-bc20-e2fb83688473",
   "metadata": {
    "gather": {
     "logged": 1670431853887
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824904\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>798026</th>\n",
       "      <td>1384.0</td>\n",
       "      <td>1340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604683</th>\n",
       "      <td>933.0</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442230</th>\n",
       "      <td>693.0</td>\n",
       "      <td>1343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708453</th>\n",
       "      <td>1149.0</td>\n",
       "      <td>1307.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70935</th>\n",
       "      <td>150.0</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         input  output\n",
       "798026  1384.0  1340.0\n",
       "604683   933.0   252.0\n",
       "442230   693.0  1343.0\n",
       "708453  1149.0  1307.0\n",
       "70935    150.0   288.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsampling_treshold = 0.02 #0.000002\n",
    "emb3 = subsample(emb2, subsampling_treshold).sample(frac=1) # to make sure that we shuffle correctly\n",
    "print(len(emb3))\n",
    "emb3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b80a3ed2-82d6-49fe-ae8d-99861747e20f",
   "metadata": {
    "gather": {
     "logged": 1670431867067
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n"
     ]
    }
   ],
   "source": [
    "observed_items = emb3.values.flatten() # returns all observed items in a list \n",
    "vocab_size = len(set(observed_items)) # Will need to change that for number of distinct items in dataset\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "607a3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(emb3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50687f-88a7-4b13-9989-54b8d0c1d5b8",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 2.3 Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f035e4e-714f-4c13-bd43-adbc78102844",
   "metadata": {
    "gather": {
     "logged": 1670432920828
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "k=5\n",
    "pwr_val=1/4\n",
    "\n",
    "neg_samp_df, targets, contexts, labels = generate_negative_samples(emb3.sample(frac=1), k, pwr_val)\n",
    "\n",
    "## FREE UP MEMORY\n",
    "del emb, emb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b06a17c6-7fbb-422f-9818-9d54b463be82",
   "metadata": {
    "gather": {
     "logged": 1670432937925
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "targets_tf = tf.convert_to_tensor(targets)\n",
    "contexts_tf = tf.convert_to_tensor(contexts)\n",
    "labels_tf = tf.convert_to_tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70d7cb",
   "metadata": {},
   "source": [
    "### 3. Generate Tensorflow Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "161a005a-a235-4e0a-8926-816a2fc73758",
   "metadata": {
    "gather": {
     "logged": 1670435780992
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:  742413 Test Size: 82491\n"
     ]
    }
   ],
   "source": [
    "# Just disables the warning, doesn't take advantage of AVX/FMA to run faster\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # this is to ignore the error message: assuming you are running on GPU it is not necessary to see the error message\n",
    "\n",
    "BATCH_SIZE = 30  ### --> HYPER-PARAMETER TO FINE TUNE\n",
    "BUFFER_SIZE = 10000\n",
    "TRAIN_PERC = .9 # Percentage of samples in train\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets_tf, contexts_tf), labels_tf))\n",
    "n_samples = len(list(dataset))\n",
    "train_size = int(n_samples * TRAIN_PERC)\n",
    "test_size = n_samples - train_size\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)\n",
    "\n",
    "print(\"Train Size: \", len(list(train_dataset)), \"Test Size:\", len(list(test_dataset)))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0b44508-e618-4637-b7c8-107723a736db",
   "metadata": {
    "gather": {
     "logged": 1670435781140
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "filename_ = str(min_count_rare_word_pruning) + \"_\" + str(subsampling_treshold) + \"_\" + str(vocab_size) + \"_\" + str(k) + \"_\" + str(pwr_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f292a1",
   "metadata": {},
   "source": [
    "### 4. Define the Hyperparameters of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd800296",
   "metadata": {},
   "source": [
    "- LEARNING_RATES: The learning rate of the optimizer\n",
    "- EMBEDDING_DIMS: The dimension of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aggressive-label",
   "metadata": {
    "gather": {
     "logged": 1670435781523
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIMS = [5,6,7] #[10]\n",
    "LEARNING_RATES = [.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "generous-committee",
   "metadata": {
    "gather": {
     "logged": 1669689242657
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'word2_vec_2/w2v_embedding/embedding_lookup' defined at (most recent call last):\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\AC226827\\AppData\\Local\\Temp\\ipykernel_5876\\1037852174.py\", line 32, in <cell line: 2>\n      results = word2vec.fit(train_dataset,\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\OneDrive - Air Canada\\Documents\\ML AI Content\\2022\\Recommender Systems\\to_load_gitub\\utils.py\", line 239, in call\n      word_emb = self.target_embedding(target)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\layers\\core\\embedding.py\", line 208, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'word2_vec_2/w2v_embedding/embedding_lookup'\nindices[0] = 900 is not in [0, 740)\n\t [[{{node word2_vec_2/w2v_embedding/embedding_lookup}}]] [Op:__inference_train_function_4966398]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m es \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     27\u001b[0m             min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m     28\u001b[0m             patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     29\u001b[0m             mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     30\u001b[0m             restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m---> 32\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mword2vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m                     \n\u001b[0;32m     38\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(embedding_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(learning_rate)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m train_loss_dict[model_name] \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'word2_vec_2/w2v_embedding/embedding_lookup' defined at (most recent call last):\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\AC226827\\AppData\\Local\\Temp\\ipykernel_5876\\1037852174.py\", line 32, in <cell line: 2>\n      results = word2vec.fit(train_dataset,\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\OneDrive - Air Canada\\Documents\\ML AI Content\\2022\\Recommender Systems\\to_load_gitub\\utils.py\", line 239, in call\n      word_emb = self.target_embedding(target)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AC226827\\Anaconda3\\lib\\site-packages\\keras\\layers\\core\\embedding.py\", line 208, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'word2_vec_2/w2v_embedding/embedding_lookup'\nindices[0] = 900 is not in [0, 740)\n\t [[{{node word2_vec_2/w2v_embedding/embedding_lookup}}]] [Op:__inference_train_function_4966398]"
     ]
    }
   ],
   "source": [
    "save_embeddings = True\n",
    "for embedding_dim in EMBEDDING_DIMS: \n",
    "    for learning_rate in LEARNING_RATES: \n",
    "\n",
    "        train_loss_dict = {}\n",
    "        train_accuracy_dict = {}\n",
    "        test_results_dict = {}\n",
    "\n",
    "        word2vec = Word2Vec(vocab_size, embedding_dim, 5)\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "                learning_rate=learning_rate,\n",
    "                beta_1=0.9,\n",
    "                beta_2=0.999,\n",
    "                epsilon=1e-07,\n",
    "                amsgrad=False,\n",
    "                name='Adam',\n",
    "            )\n",
    "            \n",
    "        word2vec.compile(optimizer=opt,\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "        \n",
    "        # Callbacks\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                    min_delta=0.001,\n",
    "                    patience=2,\n",
    "                    mode='max',\n",
    "                    restore_best_weights=True) \n",
    "        \n",
    "        results = word2vec.fit(train_dataset, \n",
    "                                epochs=10, \n",
    "                                callbacks=[tensorboard_callback, es],\n",
    "                                validation_data=test_dataset\n",
    "                            )                     \n",
    "        \n",
    "        model_name = f\"{str(embedding_dim)}-{str(learning_rate)}\"\n",
    "        train_loss_dict[model_name] = results.history[\"loss\"]\n",
    "        train_accuracy_dict[model_name] = results.history[\"accuracy\"]\n",
    "        \n",
    "        test_results = word2vec.evaluate(test_dataset, callbacks=[tensorboard_callback])\n",
    "        test_results_dict[model_name] = test_results \n",
    "\n",
    "        model_id = filename_ + '_EMB_DIM_' + str(embedding_dim) + \"_LR_\" + str(learning_rate)\n",
    "\n",
    "        logs_dict = {model_id:{'loss':train_loss_dict, \n",
    "                        'accuracy':train_accuracy_dict,\n",
    "                        'test':test_results_dict\n",
    "                                    }\n",
    "                    }\n",
    "\n",
    "        # Saving as a pickle file\n",
    "        local_path = './skip_gram_training_logs/' + model_id + '/'\n",
    "\n",
    "        # if the path doesn't exist\n",
    "        if not os.path.exists(local_path):\n",
    "            os.makedirs(local_path)\n",
    "        \n",
    "        with open(local_path + 'logs_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(logs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        print(model_name, \"Test Loss\", test_results[0], \"Test Accuracy\", test_results[1])\n",
    "\n",
    "        if save_embeddings:\n",
    "            weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "            df_embed = pd.DataFrame(weights, index=vocab.keys())\n",
    "\n",
    "            with open(local_path + 'df_embed.pickle', 'wb') as handle:\n",
    "                pickle.dump(df_embed, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c26ee-dcfe-4b57-b31f-9b8efaf241d1",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe5e72ec-5908-427c-bff8-ad86ee9cfd58",
   "metadata": {
    "gather": {
     "logged": 1669689242789
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "df_embed = pd.DataFrame(weights, index = vocab.keys())\n",
    "df_embed.to_csv(\"./df_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9b7bee-fbdb-4551-8eab-673dff92b966",
   "metadata": {
    "gather": {
     "logged": 1669744752671
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "905"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb7511f8-2d73-474e-91e5-406261f4ff67",
   "metadata": {
    "gather": {
     "logged": 1669744753958
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aae</th>\n",
       "      <th>aal</th>\n",
       "      <th>aar</th>\n",
       "      <th>abe</th>\n",
       "      <th>abj</th>\n",
       "      <th>abq</th>\n",
       "      <th>abv</th>\n",
       "      <th>abx</th>\n",
       "      <th>abz</th>\n",
       "      <th>aca</th>\n",
       "      <th>...</th>\n",
       "      <th>zih</th>\n",
       "      <th>zlo</th>\n",
       "      <th>znz</th>\n",
       "      <th>zqn</th>\n",
       "      <th>zrh</th>\n",
       "      <th>zsa</th>\n",
       "      <th>zth</th>\n",
       "      <th>zuh</th>\n",
       "      <th>zws</th>\n",
       "      <th>ıst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aae</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.042086</td>\n",
       "      <td>-0.088361</td>\n",
       "      <td>0.122154</td>\n",
       "      <td>0.589810</td>\n",
       "      <td>0.147615</td>\n",
       "      <td>0.355155</td>\n",
       "      <td>-0.390729</td>\n",
       "      <td>-0.117424</td>\n",
       "      <td>0.096971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049498</td>\n",
       "      <td>0.195215</td>\n",
       "      <td>0.302740</td>\n",
       "      <td>-0.248104</td>\n",
       "      <td>0.479312</td>\n",
       "      <td>-0.037291</td>\n",
       "      <td>-0.019002</td>\n",
       "      <td>0.228834</td>\n",
       "      <td>0.084672</td>\n",
       "      <td>0.742451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aal</th>\n",
       "      <td>-0.042086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982955</td>\n",
       "      <td>0.283006</td>\n",
       "      <td>0.195596</td>\n",
       "      <td>-0.099365</td>\n",
       "      <td>-0.143011</td>\n",
       "      <td>0.288958</td>\n",
       "      <td>0.380987</td>\n",
       "      <td>-0.324010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332027</td>\n",
       "      <td>-0.253191</td>\n",
       "      <td>0.209240</td>\n",
       "      <td>0.275961</td>\n",
       "      <td>0.415281</td>\n",
       "      <td>-0.210217</td>\n",
       "      <td>-0.018955</td>\n",
       "      <td>-0.092794</td>\n",
       "      <td>0.169026</td>\n",
       "      <td>0.024954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aar</th>\n",
       "      <td>-0.088361</td>\n",
       "      <td>0.982955</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.238523</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.010415</td>\n",
       "      <td>-0.096053</td>\n",
       "      <td>0.332344</td>\n",
       "      <td>0.463156</td>\n",
       "      <td>-0.255200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280595</td>\n",
       "      <td>-0.173365</td>\n",
       "      <td>0.136918</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>0.343346</td>\n",
       "      <td>-0.250485</td>\n",
       "      <td>-0.055906</td>\n",
       "      <td>-0.042153</td>\n",
       "      <td>0.064756</td>\n",
       "      <td>-0.037553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abe</th>\n",
       "      <td>0.122154</td>\n",
       "      <td>0.283006</td>\n",
       "      <td>0.238523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.404975</td>\n",
       "      <td>0.157857</td>\n",
       "      <td>-0.028761</td>\n",
       "      <td>0.057125</td>\n",
       "      <td>-0.238542</td>\n",
       "      <td>0.341483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197930</td>\n",
       "      <td>0.240018</td>\n",
       "      <td>-0.202823</td>\n",
       "      <td>-0.113170</td>\n",
       "      <td>0.113985</td>\n",
       "      <td>0.384829</td>\n",
       "      <td>-0.310915</td>\n",
       "      <td>0.188630</td>\n",
       "      <td>0.064168</td>\n",
       "      <td>-0.020300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abj</th>\n",
       "      <td>0.589810</td>\n",
       "      <td>0.195596</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.404975</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.140140</td>\n",
       "      <td>0.414363</td>\n",
       "      <td>-0.034936</td>\n",
       "      <td>0.248225</td>\n",
       "      <td>0.293012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310090</td>\n",
       "      <td>0.244104</td>\n",
       "      <td>0.285773</td>\n",
       "      <td>-0.005069</td>\n",
       "      <td>0.648946</td>\n",
       "      <td>0.259281</td>\n",
       "      <td>0.317179</td>\n",
       "      <td>0.309338</td>\n",
       "      <td>0.344633</td>\n",
       "      <td>0.521967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 905 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aae       aal       aar       abe       abj       abq       abv  \\\n",
       "aae  1.000000 -0.042086 -0.088361  0.122154  0.589810  0.147615  0.355155   \n",
       "aal -0.042086  1.000000  0.982955  0.283006  0.195596 -0.099365 -0.143011   \n",
       "aar -0.088361  0.982955  1.000000  0.238523  0.138889  0.010415 -0.096053   \n",
       "abe  0.122154  0.283006  0.238523  1.000000  0.404975  0.157857 -0.028761   \n",
       "abj  0.589810  0.195596  0.138889  0.404975  1.000000  0.140140  0.414363   \n",
       "\n",
       "          abx       abz       aca  ...       zih       zlo       znz  \\\n",
       "aae -0.390729 -0.117424  0.096971  ...  0.049498  0.195215  0.302740   \n",
       "aal  0.288958  0.380987 -0.324010  ... -0.332027 -0.253191  0.209240   \n",
       "aar  0.332344  0.463156 -0.255200  ... -0.280595 -0.173365  0.136918   \n",
       "abe  0.057125 -0.238542  0.341483  ...  0.197930  0.240018 -0.202823   \n",
       "abj -0.034936  0.248225  0.293012  ...  0.310090  0.244104  0.285773   \n",
       "\n",
       "          zqn       zrh       zsa       zth       zuh       zws       ıst  \n",
       "aae -0.248104  0.479312 -0.037291 -0.019002  0.228834  0.084672  0.742451  \n",
       "aal  0.275961  0.415281 -0.210217 -0.018955 -0.092794  0.169026  0.024954  \n",
       "aar  0.346667  0.343346 -0.250485 -0.055906 -0.042153  0.064756 -0.037553  \n",
       "abe -0.113170  0.113985  0.384829 -0.310915  0.188630  0.064168 -0.020300  \n",
       "abj -0.005069  0.648946  0.259281  0.317179  0.309338  0.344633  0.521967  \n",
       "\n",
       "[5 rows x 905 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.spatial as sp\n",
    "\n",
    "df_embed.sort_index(inplace=True)\n",
    "#computing the cosine between the destinations\n",
    "cosines = 1 - sp.distance.cdist(df_embed.values, df_embed.values, 'cosine')\n",
    "\n",
    "#creating a dataframe from the cosine\n",
    "df_cosines = pd.DataFrame(cosines, columns = df_embed.index, index = df_embed.index)\n",
    "\n",
    "df_cosines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed0e8e4d-9fa3-4d59-a9d7-eb27e822d026",
   "metadata": {
    "gather": {
     "logged": 1669744759613
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>cosine</th>\n",
       "      <th>similar_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aae</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>aae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aae</td>\n",
       "      <td>0.980399</td>\n",
       "      <td>bja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aae</td>\n",
       "      <td>0.977457</td>\n",
       "      <td>czl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aae</td>\n",
       "      <td>0.971211</td>\n",
       "      <td>orn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aae</td>\n",
       "      <td>0.845885</td>\n",
       "      <td>hah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  city    cosine similar_city\n",
       "0  aae  1.000000          aae\n",
       "1  aae  0.980399          bja\n",
       "2  aae  0.977457          czl\n",
       "3  aae  0.971211          orn\n",
       "4  aae  0.845885          hah"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the top 10 most similar cities for each city\n",
    "big_arr = []\n",
    "for row in df_cosines.iterrows():\n",
    "    \n",
    "    top10 = row[1].nlargest(10)\n",
    "    \n",
    "    for i in range(0, len(top10.values)):\n",
    "        big_arr.append([top10.name, top10.values[i], top10.index.values[i]])\n",
    "    \n",
    "    \n",
    "df_city_similarity = pd.DataFrame(big_arr, columns=['city', 'cosine', 'similar_city'])\n",
    "df_city_similarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88077606-f56a-4be4-a15a-d7a17bf04f36",
   "metadata": {
    "gather": {
     "logged": 1669744972610
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>cosine</th>\n",
       "      <th>similar_city</th>\n",
       "      <th>airp_cd</th>\n",
       "      <th>city_name</th>\n",
       "      <th>COUNTRY_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aae</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>aae</td>\n",
       "      <td>aae</td>\n",
       "      <td>ANNABA</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aae</td>\n",
       "      <td>0.980399</td>\n",
       "      <td>bja</td>\n",
       "      <td>bja</td>\n",
       "      <td>BEJAIA</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aae</td>\n",
       "      <td>0.977457</td>\n",
       "      <td>czl</td>\n",
       "      <td>czl</td>\n",
       "      <td>CONSTANTINE</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aae</td>\n",
       "      <td>0.971211</td>\n",
       "      <td>orn</td>\n",
       "      <td>orn</td>\n",
       "      <td>ORAN</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aae</td>\n",
       "      <td>0.845885</td>\n",
       "      <td>hah</td>\n",
       "      <td>hah</td>\n",
       "      <td>MORONI</td>\n",
       "      <td>COMOROS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  city    cosine similar_city airp_cd    city_name COUNTRY_NAME\n",
       "0  aae  1.000000          aae     aae       ANNABA      ALGERIA\n",
       "1  aae  0.980399          bja     bja       BEJAIA      ALGERIA\n",
       "2  aae  0.977457          czl     czl  CONSTANTINE      ALGERIA\n",
       "3  aae  0.971211          orn     orn         ORAN      ALGERIA\n",
       "4  aae  0.845885          hah     hah       MORONI      COMOROS"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_similarity = pd.merge(df_city_similarity, ref_airpt[['airp_cd', 'city_name', 'COUNTRY_NAME']], how='left', left_on='similar_city', right_on='airp_cd')\n",
    "df_city_similarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cd20589-f064-4804-9e27-5b8d0b2953f6",
   "metadata": {
    "gather": {
     "logged": 1669745919524
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>cosine</th>\n",
       "      <th>similar_city</th>\n",
       "      <th>airp_cd</th>\n",
       "      <th>city_name</th>\n",
       "      <th>COUNTRY_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6600</th>\n",
       "      <td>ptp</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>ptp</td>\n",
       "      <td>ptp</td>\n",
       "      <td>POINTE A PITRE</td>\n",
       "      <td>GUADELOUPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6601</th>\n",
       "      <td>ptp</td>\n",
       "      <td>0.965177</td>\n",
       "      <td>fdf</td>\n",
       "      <td>fdf</td>\n",
       "      <td>FORT DE FRANCE</td>\n",
       "      <td>MARTINIQUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6602</th>\n",
       "      <td>ptp</td>\n",
       "      <td>0.955561</td>\n",
       "      <td>pnr</td>\n",
       "      <td>pnr</td>\n",
       "      <td>POINTE NOIRE</td>\n",
       "      <td>CONGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6603</th>\n",
       "      <td>ptp</td>\n",
       "      <td>0.926858</td>\n",
       "      <td>cur</td>\n",
       "      <td>cur</td>\n",
       "      <td>CURACAO</td>\n",
       "      <td>CURACAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6604</th>\n",
       "      <td>ptp</td>\n",
       "      <td>0.904372</td>\n",
       "      <td>bon</td>\n",
       "      <td>bon</td>\n",
       "      <td>BONAIRE</td>\n",
       "      <td>BONAIRE, SAINT EUSTATIUS &amp; SABA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6605</th>\n",
       "      <td>ptp</td>\n",
       "      <td>0.871076</td>\n",
       "      <td>aua</td>\n",
       "      <td>aua</td>\n",
       "      <td>ARUBA</td>\n",
       "      <td>ARUBA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6606</th>\n",
       "      <td>ptp</td>\n",
       "      <td>0.863810</td>\n",
       "      <td>dom</td>\n",
       "      <td>dom</td>\n",
       "      <td>DOMINICA</td>\n",
       "      <td>DOMINICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6607</th>\n",
       "      <td>ptp</td>\n",
       "      <td>0.857145</td>\n",
       "      <td>sxm</td>\n",
       "      <td>sxm</td>\n",
       "      <td>ST. MAARTEN</td>\n",
       "      <td>SINT MAARTEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6608</th>\n",
       "      <td>ptp</td>\n",
       "      <td>0.848064</td>\n",
       "      <td>ctg</td>\n",
       "      <td>ctg</td>\n",
       "      <td>CARTAGENA</td>\n",
       "      <td>COLOMBIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6609</th>\n",
       "      <td>ptp</td>\n",
       "      <td>0.847514</td>\n",
       "      <td>anu</td>\n",
       "      <td>anu</td>\n",
       "      <td>ANTIGUA</td>\n",
       "      <td>ANTIGUA AND BARBUDA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     city    cosine similar_city airp_cd       city_name  \\\n",
       "6600  ptp  1.000000          ptp     ptp  POINTE A PITRE   \n",
       "6601  ptp  0.965177          fdf     fdf  FORT DE FRANCE   \n",
       "6602  ptp  0.955561          pnr     pnr    POINTE NOIRE   \n",
       "6603  ptp  0.926858          cur     cur         CURACAO   \n",
       "6604  ptp  0.904372          bon     bon         BONAIRE   \n",
       "6605  ptp  0.871076          aua     aua           ARUBA   \n",
       "6606  ptp  0.863810          dom     dom        DOMINICA   \n",
       "6607  ptp  0.857145          sxm     sxm     ST. MAARTEN   \n",
       "6608  ptp  0.848064          ctg     ctg       CARTAGENA   \n",
       "6609  ptp  0.847514          anu     anu         ANTIGUA   \n",
       "\n",
       "                         COUNTRY_NAME  \n",
       "6600                       GUADELOUPE  \n",
       "6601                       MARTINIQUE  \n",
       "6602                            CONGO  \n",
       "6603                          CURACAO  \n",
       "6604  BONAIRE, SAINT EUSTATIUS & SABA  \n",
       "6605                            ARUBA  \n",
       "6606                         DOMINICA  \n",
       "6607                     SINT MAARTEN  \n",
       "6608                         COLOMBIA  \n",
       "6609              ANTIGUA AND BARBUDA  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this code can be used to find the exact city_filename for a city. For instance, for London\n",
    "df_city_similarity[df_city_similarity.city.str.contains('mad')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18815369-e5f8-4164-9cb9-c08e0125f238",
   "metadata": {
    "gather": {
     "logged": 1669715777246
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________a________________________________________________________________________________________________\n",
      "\n",
      " FOR psp :\n",
      "\n",
      " DESTINATION METADATA RETURNS:\n",
      "     similar_city\n",
      "6590          psp\n",
      "6591          phx\n",
      "6592          san\n",
      "6593          ont\n",
      "6594          tus\n",
      "6595          bur\n",
      "6596          sfo\n",
      "6597          saf\n",
      "6598          sna\n",
      "6599          slc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CANCUN\n",
    "# MONTEGO_BAY\n",
    "# LYON\n",
    "\n",
    "city='psp'\n",
    "\n",
    "print(\"___________________________________________________________a________________________________________________________________________________________________\")\n",
    "print(\"\")\n",
    "print(\" FOR \" + city + ' :')\n",
    "print(\"\")\n",
    "#you can then use the exact city_filename to look for the top 10 most similar cities\n",
    "print(\" Destination Metadata returns:\".upper())\n",
    "print(df_city_similarity[df_city_similarity.city==city][['similar_city']])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119901ba-a9ea-46c9-927e-67a695992761",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "weights = {}\n",
    "\n",
    "for idx, row in df_embed2.iterrows():\n",
    "    weights[idx] = row.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38588e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for key, val in weights.items(): # for index, word in enumerate(vocab):\n",
    "  vec = weights[key]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(key + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
